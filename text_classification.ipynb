{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "This notebook provides functions to run and evaluate a text classifier, with a particular focus on metrics that assess the dataset imbalance and the prediction bias.\n",
    "\n",
    "The first part of the notebook contains the definition of the functions, followed by an execution on the selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'reuters'             # 'webscope_r4' or 'reuters'\n",
    "select_only_labelled = True     # if True, only documents with at least 1 label assigned will be considered\n",
    "do_text_preprocessing = False   # if True, stopword-removal, lower-casing and stemming are applied to the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, random, re, unidecode, os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from gensim.parsing import preprocessing as pproc\n",
    "\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the proportion of samples for each unique label in ``labels_per_sample``.\n",
    "\n",
    "Parameters:\n",
    "- ``labels_per_sample``: array-like object, where the i-th element indicates the label of the i-th sample\n",
    "\n",
    "Returns:\n",
    "- a dictionary, where keys are labels and values are real numbers in the 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_frequencies(labels_per_sample):\n",
    "    \n",
    "    all_labels = []\n",
    "    for labels in labels_per_sample:\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(all_labels, return_counts=True)\n",
    "    occurrences_dict = dict(zip(unique_labels, label_counts))\n",
    "    frequencies_dict = {l: occurrences_dict[l] / len(labels_per_sample) for l in occurrences_dict}\n",
    "\n",
    "    return frequencies_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as ``calculate_label_frequencies``, but the values are absolute numbers of occurrences of the label, instead of proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_occurrences(labels_per_sample):\n",
    "    \n",
    "    all_labels = []\n",
    "    for labels in labels_per_sample:\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    unique_labels, label_counts = np.unique(all_labels, return_counts=True)\n",
    "    occurrences_dict = dict(zip(unique_labels, label_counts))\n",
    "\n",
    "    return occurrences_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized version of the coverage error (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error).\n",
    "\n",
    "The normalized value is obtained by the formula:\n",
    "\n",
    "```normalized_cov_err = (cov_err - best_possible_cov_err) / (worst_possible_cov_err - best_possible_cov_err)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_coverage_error(y_true, y_score):\n",
    "    cov_err = coverage_error(y_true, y_score)\n",
    "    best_possible_cov_err = np.average([sum(yt) for yt in y_true])\n",
    "    worst_possible_cov_err = len(y_true[0])\n",
    "    \n",
    "    normalized_cov_err = (cov_err - best_possible_cov_err) / (worst_possible_cov_err - best_possible_cov_err)\n",
    "    \n",
    "    return normalized_cov_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for ``avg_exposure_aggregate``. Computes the average exposure assigned by the ranking to the ground truth labels. Follows the definition of \"exposure\" by Singh et al., 2018.\n",
    "\n",
    "Params:\n",
    "- ``y_true``: ground-truth labels associated with a specific sample (one-hot sparse representation)\n",
    "- ``y_score``: predicted scores for each label\n",
    "- ``normalize``: if True, the exposure is normalized with the formula ``avg_exp = (avg_exp - worst_possible_exp) / (best_possible_exp-worst_possible_exp)``\n",
    "\n",
    "Returns:\n",
    "- a real value representing the average exposure of the ground-truth labels for one specific sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _avg_exposure_single(y_true, y_score, normalize=True):\n",
    "    \n",
    "    true_labels = [l for l,v in enumerate(y_true) if v]\n",
    "    ranked_labels = sorted([(l,s) for l,s in enumerate(y_score)], reverse=True, key=lambda x:x[-1])\n",
    "    \n",
    "    avg_exp = 0\n",
    "    for j,(l,s) in enumerate(ranked_labels):\n",
    "        if l in true_labels:\n",
    "            avg_exp += 1 / math.log2(2+j)\n",
    "    \n",
    "    if normalize:\n",
    "        best_possible_exp = sum([(1 / math.log2(2+j)) for j in range(len(true_labels))])\n",
    "        worst_possible_exp = sum([(1 / math.log2(2+j)) for j in range(len(ranked_labels)-1,len(ranked_labels)-len(true_labels)-1,-1)])\n",
    "        \n",
    "        try:\n",
    "            avg_exp = (avg_exp - worst_possible_exp) / (best_possible_exp-worst_possible_exp)\n",
    "        except ZeroDivisionError:\n",
    "            avg_exp = 0\n",
    "        \n",
    "    return avg_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes, for each sample, the exposure assigned to the true labels by the rankings of labels. See also ``_avg_exposure_single``.\n",
    "\n",
    "Params:\n",
    "- ``y_true``: array-like object where the i-th element contains the ground-truth labels associated with the i-th sample, represented in a one-hot sparse vector\n",
    "- ``y_score``: array-like object where the i-th element contains the predicted scores for all labels associated with the i-th sample\n",
    "\n",
    "Returns:\n",
    "- real value representing the exposure, averaged over all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_exposure_aggregate(y_true, y_score):\n",
    "    \n",
    "    avg_exp_aggr = 0\n",
    "    for yt, ys in zip(y_true, y_score):\n",
    "        avg_exp_aggr += _avg_exposure_single(yt, ys)\n",
    "    avg_exp_aggr /= len(y_true)\n",
    "    return avg_exp_aggr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a variety of metrics given a ground truth and a series of predictions. The metrics included in the analysis are:\n",
    "- mean imbalance ratio\n",
    "- coefficient of variation of the imbalance ratio\n",
    "- label ranking average precision\n",
    "- label ranking loss\n",
    "- normalized coverage error\n",
    "- averaged exposure\n",
    "- balanced accuracy\n",
    "- precision\n",
    "- recall\n",
    "- f-score\n",
    "- prediction bias coefficient (here ``fscore_correlation``).\n",
    "\n",
    "See the paper \"Evaluating the Prediction Bias Induced by Label Imbalance in Multi-label Classification\" for reference on the metrics.\n",
    "\n",
    "Params:\n",
    "- ``true_labels``: array-like object where the i-th element contains the ground-truth labels associated with the i-th sample\n",
    "- ``true_labels_sparse``: same as ``true_labels``, but the labels for each sample are represented through a sparse one-hot vector\n",
    "- ``predicted_labels``: array-like object where the i-th element contains the predicted labels associated with the i-th sample\n",
    "- ``predicted_scores``: array-like object where the i-th element contains the scores assigned to all labels associated with the i-th sample\n",
    "- ``training_frequencies_dict``: dictionary where keys are labels and values are the proportion of samples in the training set associated with that label\n",
    "- ``test_frequencies_dict``: dictionary where keys are labels and values are the proportion of samples in the test set associated with that label\n",
    "\n",
    "Returns:\n",
    "- a dictionary where keys are the names of the metrics and values are the obtained scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_labels, true_labels_sparse, predicted_labels, predicted_scores,\n",
    "                    training_frequencies_dict, test_frequencies_dict):\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    IR_per_label = [max(training_frequencies_dict.values()) / training_frequencies_dict[l] for l in training_frequencies_dict]\n",
    "    meanIR = np.average(IR_per_label)\n",
    "    stdevIR = np.std(IR_per_label)\n",
    "    CVIR = stdevIR / meanIR\n",
    "    \n",
    "    metrics['meanIR'] = meanIR\n",
    "    metrics['CVIR'] = CVIR\n",
    "    metrics['lraps'] = label_ranking_average_precision_score(true_labels_sparse, predicted_scores)\n",
    "    metrics['lrl'] = label_ranking_loss(true_labels_sparse, predicted_scores)\n",
    "    metrics['cov_err'] = normalized_coverage_error(true_labels_sparse, predicted_scores)\n",
    "    metrics['avg_exp'] = avg_exposure_aggregate(true_labels_sparse, predicted_scores)\n",
    "\n",
    "    balanced_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fscores = []\n",
    "    training_frequencies = []\n",
    "    test_frequencies = []\n",
    "    label_names = []\n",
    "    for current_label in sorted(training_frequencies_dict, key=training_frequencies_dict.get, reverse=False):\n",
    "        if current_label in test_frequencies_dict:# and current_label not in ('acq', 'earn'):\n",
    "            true_labels_binary = [1 if current_label in labels else 0 for labels in true_labels]\n",
    "            predicted_labels_binary = [1 if current_label in labels else 0 for labels in predicted_labels]\n",
    "            \n",
    "            balanced_accuracies.append(balanced_accuracy_score(true_labels_binary, predicted_labels_binary))\n",
    "            precisions.append(precision_score(true_labels_binary, predicted_labels_binary, zero_division=0))\n",
    "            recalls.append(recall_score(true_labels_binary, predicted_labels_binary, zero_division=0))\n",
    "            fscores.append(f1_score(true_labels_binary, predicted_labels_binary, zero_division=0))\n",
    "            training_frequencies.append(training_frequencies_dict[current_label])\n",
    "            test_frequencies.append(test_frequencies_dict[current_label])\n",
    "            label_names.append(current_label)\n",
    "\n",
    "    metrics['avg_balanced_accuracy'] = np.average(balanced_accuracies)\n",
    "    metrics['avg_precision'] = np.average(precisions)\n",
    "    metrics['avg_recall'] = np.average(recalls)\n",
    "    metrics['avg_fscore'] = np.average(fscores)\n",
    "\n",
    "    (metrics['precision_correlation'], \n",
    "     metrics['asymptotic_precision']) = plot_correlation(training_frequencies, precisions, xlabel='Frequency in Training Set', ylabel='Precision', label_names=label_names)\n",
    "\n",
    "    (metrics['recall_correlation'], \n",
    "     metrics['asymptotic_recall']) = plot_correlation(training_frequencies, recalls, xlabel='Frequency in Training Set', ylabel='Recall', label_names=label_names)\n",
    "\n",
    "    (metrics['fscore_correlation'], \n",
    "     metrics['asymptotic_fscore']) = plot_correlation(training_frequencies, fscores, xlabel='Frequency in Training Set', ylabel='F-score', label_names=label_names)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a scatter plot of the values of ``metric_array_y`` (y-axis) against the values of ``metrics_array_x`` (x-axis) and returns the Spearman's correlation between the two arrays of quantities, together with the expected value of y for x == 0, according to the interpolation function. \n",
    "\n",
    "Params:\n",
    "- ``metric_array_x``: array-like object with the values for the x-coordinates\n",
    "- ``metric_array_y``: array-like object with the values for the y-coordinates\n",
    "- ``polynomial_degree``: degree of the interpolation, representing the trend line (can be 1, 2 or 3)\n",
    "- ``xlabel``: label for the x-axis\n",
    "- ``ylabel``: label for the y-axis\n",
    "- ``label_names``: array-like object where the i-th element contains the label to print for the point at ``(metric_array_x[i], metric_array_y[i])`` \n",
    "\n",
    "Returns:\n",
    "- Spearman's correlation coefficient between ``metric_array_x`` and ``metric_array_y``\n",
    "- expected value for y when x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(metric_array_x, metric_array_y, print_plot=True, \n",
    "                     polynomial_degree=1, xlabel='', ylabel='', label_names=[]):\n",
    "    assert polynomial_degree >= 1 and polynomial_degree <= 3\n",
    "    \n",
    "    z_1 = np.polyfit(metric_array_x, metric_array_y, 1)\n",
    "    p_1 = np.poly1d(z_1)\n",
    "    \n",
    "    z_2 = np.polyfit(metric_array_x, metric_array_y, 2)\n",
    "    p_2 = np.poly1d(z_2)\n",
    "    \n",
    "    z_3 = np.polyfit(metric_array_x, metric_array_y, 3)\n",
    "    p_3 = np.poly1d(z_3)\n",
    "    \n",
    "    if polynomial_degree == 1:\n",
    "        asymptotic_value = p_1(0)\n",
    "    elif polynomial_degree == 2:\n",
    "        asymptotic_value = p_2(0)\n",
    "    elif polynomial_degree == 3:\n",
    "        asymptotic_value = p_3(0)\n",
    "    \n",
    "    correlation, p_value = stats.spearmanr(metric_array_x, metric_array_y)\n",
    "    \n",
    "    if print_plot:\n",
    "        fig, ax = plt.subplots(figsize=(15,8))\n",
    "        title = ('Spearman Correlation Coefficient: ' + str(round(correlation,2)))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.scatter(metric_array_x, metric_array_y)\n",
    "        for x, y, l in zip(metric_array_x, metric_array_y, label_names):\n",
    "            ax.annotate(l, (x, y), fontsize=22)\n",
    "        ax.plot(metric_array_x, p_1(metric_array_x),\"r--\")\n",
    "        plt.rc('axes', labelsize=24)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=24)    # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=24)    # fontsize of the tick labels\n",
    "        plt.show()\n",
    "    \n",
    "    return correlation, asymptotic_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply standard pre-processing techniques to a text and return the normalized string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(string, lowercase=True, remove_stopwords=True, stemming=False):\n",
    "    \n",
    "    string = unidecode.unidecode(string)\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    abbreviations = re.findall(r'(?:[a-z]\\.)+', string)\n",
    "    for abbr in abbreviations:\n",
    "        string = string.replace(abbr, abbr.replace('.',''))\n",
    "    string = pproc.strip_punctuation2(string)\n",
    "    if remove_stopwords:\n",
    "        string = pproc.remove_stopwords(string)\n",
    "    if stemming:\n",
    "        string = pproc.stem_text(string)\n",
    "    string = string.strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the samples in the appropriate format for the spaCy's TextCategorizer.\n",
    "\n",
    "Params:\n",
    "- ``samples``: list of (text, labels) tuples\n",
    "- ``unique_labels``: set of all possible unique labels\n",
    "\n",
    "Returns:\n",
    "- array-like object with samples ready to be processed by spaCy's TextCategorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cats(samples, unique_labels):\n",
    "    new_samples = []\n",
    "    for text,labels in samples:\n",
    "        cats = {unique_l:(unique_l in labels) for unique_l in unique_labels}\n",
    "        new_samples.append((text, {'cats':cats}))\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a list of lists of labels (one per each sample) into an array-like object representing the labels associated with each sample in a one-hot sparse encoding.\n",
    "\n",
    "Params:\n",
    "- ``labels_per_sample``: list of lists of labels, one per sample\n",
    "- ``unique_labels``: set of all possible unique labels\n",
    "\n",
    "Returns:\n",
    "- an array-like sparse representation of labels per sample, in a one-hot encoding fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_label_representations(labels_per_sample, unique_labels):\n",
    "    \n",
    "    sparse_labels_per_sample = []\n",
    "    for labels in labels_per_sample:\n",
    "        sparse_labels = np.zeros(len(unique_labels))\n",
    "        for l in labels:\n",
    "            if l in unique_labels:\n",
    "                sparse_labels[unique_labels.index(l)] = 1\n",
    "        sparse_labels_per_sample.append(sparse_labels)\n",
    "    return sparse_labels_per_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a text classifier on the provided ``training_samples``.\n",
    "\n",
    "Params:\n",
    "- ``nlp``: a spaCy language model already initialized (see https://v2.spacy.io/api/language)\n",
    "- ``training_samples``: array-like object containing samples as returned by ``prepare_cats``\n",
    "- ``n_iter``: number of iteration (epochs) to train the spaCy model\n",
    "- ``batch_size``: number of samples to be processed in the same weight-update of the model\n",
    "\n",
    "Returns:\n",
    "- a trained ``TextCategorizer`` model\n",
    "\n",
    "See https://v2.spacy.io/api/textcategorizer for further reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(nlp, training_samples, n_iter=10, batch_size=8):\n",
    "    \n",
    "    # set architecture to ensemble for better performance\n",
    "    text_classifier = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"ensemble\"})\n",
    "    nlp.add_pipe(text_classifier, last=True)\n",
    "    \n",
    "    all_training_labels = []\n",
    "    for text,labels in training_samples:\n",
    "        all_training_labels.extend(labels)\n",
    "    unique_training_labels = np.unique(all_training_labels)    \n",
    "    for label in unique_training_labels:\n",
    "        text_classifier.add_label(label)\n",
    "    \n",
    "    training_samples = prepare_cats(training_samples, text_classifier.labels)\n",
    "\n",
    "    spacy.util.fix_random_seed()\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        nlp.begin_training()\n",
    "        # Train for 10 iterations\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(training_samples)\n",
    "            # Divide examples into batches\n",
    "            for batch in spacy.util.minibatch(training_samples, size=batch_size):\n",
    "                texts = [text for text, label in batch]\n",
    "                labels = [label for text, label in batch]\n",
    "                # Update the model\n",
    "                nlp.update(docs=texts, golds=labels)\n",
    "    \n",
    "    trained_classifier = nlp.get_pipe('textcat')\n",
    "    return trained_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a previously trained ``TextCategorizer`` model on a set of ``test_samples``.\n",
    "\n",
    "Params:\n",
    "- ``nlp``: a spaCy language model already initialized (see https://v2.spacy.io/api/language)\n",
    "- ``classifier``: previously trained ``TextCategorizer`` model, as returned by ``train_classifier``\n",
    "- ``test_samples``: list of (text,labels) tuples, one for each sample in the test set\n",
    "- ``confidence_threshold``: a real value between 0 and 1; a label is assigned to a sample if its predicted score is bigger than this value\n",
    "\n",
    "Returns:\n",
    "- ``predicted_scores``: array-like object where the i-th element contains a vector of probabilities values for all labels w.r.t. the i-th element in ``test_samples``\n",
    "- ``predicted_labels``: list of lists of labels assigned to each sample in ``test_samples`` according to the ``predicted_scores`` and ``confidence_threshold``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(nlp, classifier, test_samples, confidence_threshold=0.5):\n",
    "    \n",
    "    test_docs = [nlp(descr) for descr,label in test_samples]\n",
    "    predicted_scores, tensors = classifier.predict(test_docs)\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for score in predicted_scores:\n",
    "        selected_indices = [i for i,s in enumerate(score) if s > confidence_threshold]\n",
    "        selected_labels = [l for i,l in enumerate(classifier.labels) if i in selected_indices]\n",
    "        predicted_labels.append(selected_labels)\n",
    "\n",
    "    return predicted_scores, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a k-fold cross validation on all ``samples``, which will be iteratively split into training and test sets.\n",
    "\n",
    "Params:\n",
    "- ``samples``: a list of (text, labels) tuples, with all documents to be classified\n",
    "- ``n_folds``: number of folds for the cross validation\n",
    "- ``n_training_iter``: corresponds to ``n_iter`` in ``train_classifier``\n",
    "- ``training_batch_size``: corresponds to ``batch_size`` in ``train_classifier``\n",
    "- ``confidence_threshold``: see ``train_classifier``\n",
    "- ``load_classifiers``: load already-trained classifiers (for all folds separately), if previously saved (the path is hard-coded)\n",
    "- ``save_classifiers``: save the trained classifiers for all folds separately (the path is hard-coded)\n",
    "\n",
    "Returns:\n",
    "- a 2-level dictionary with the following structure:\n",
    "    - in the first level, the keys are 'fold_1', 'fold_2', etc, and 'global'; the latter stores the values of the metrics averaged over all folds\n",
    "    - in the second level, the keys are the metrics mentioned in ``compute_metrics`` and the values are the obtained scores\n",
    "    \n",
    "    Examples: ``metrics['fold_2']['avg_precision']`` or ``metrics['global']['avg_recall']``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(samples, n_folds=5, n_training_iter=10, training_batch_size=8, confidence_threshold=0.5,\n",
    "                         load_classifiers=False, save_classifiers=False):\n",
    "    \n",
    "    if load_classifiers or save_classifiers:\n",
    "        folder_path = dataset+'_'\n",
    "        if select_only_labelled:\n",
    "            folder_path += 'only_labelled_'\n",
    "        if do_text_preprocessing:\n",
    "            folder_path += 'preprocessed_'\n",
    "        folder_path += ('n_folds_' + str(n_folds) + '_n_training_iter_' + str(n_training_iter) + \n",
    "                        '_batch_size_' + str(training_batch_size))\n",
    "    \n",
    "    assert not (load_classifiers and folder_path not in os.listdir('models'))\n",
    "    \n",
    "    samples = np.array(samples, dtype=object)\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    metrics = {}  # add metrics for each fold and averaged\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(samples):\n",
    "        nlp=spacy.load(\"en_core_web_lg\")\n",
    "        i += 1\n",
    "        print('\\nFOLD n.', i)\n",
    "        training_samples = samples[train_index]\n",
    "        test_samples = samples[test_index]\n",
    "        training_frequencies_dict = calculate_label_frequencies([labels for text,labels in training_samples])\n",
    "        test_frequencies_dict = calculate_label_frequencies([labels for text,labels in test_samples])\n",
    "        \n",
    "        if load_classifiers:\n",
    "            print('Loading classifier...')\n",
    "            text_classifier = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"ensemble\"})\n",
    "            text_classifier.from_disk('models/' + folder_path + '/classifier_fold_'+str(i))\n",
    "            print('Done.')\n",
    "        else:\n",
    "            print('Training classifier...')\n",
    "            text_classifier = train_classifier(nlp, training_samples, n_iter=n_training_iter, batch_size=training_batch_size)\n",
    "            print('Done.')\n",
    "            if save_classifiers:\n",
    "                if folder_path not in os.listdir('models/'):\n",
    "                    os.mkdir('models/'+folder_path)\n",
    "                text_classifier.to_disk('models/'+folder_path+'/classifier_fold_'+str(i))\n",
    "                \n",
    "        \n",
    "        true_labels = [labels for descr,labels in test_samples]\n",
    "        true_labels_sparse = get_sparse_label_representations(true_labels, text_classifier.labels)\n",
    "        print('Predicting...')\n",
    "        predicted_scores, predicted_labels = predict(nlp, text_classifier, test_samples, \n",
    "                                                     confidence_threshold=confidence_threshold)\n",
    "        print('Done.')\n",
    "        \n",
    "        metrics['fold_'+str(i)] = compute_metrics(true_labels, true_labels_sparse, predicted_labels, predicted_scores,\n",
    "                                                  training_frequencies_dict, test_frequencies_dict)\n",
    "       \n",
    "    metrics['global'] = {}\n",
    "    for m in ['meanIR','CVIR','lraps','lrl','cov_err','avg_exp','avg_balanced_accuracy','avg_precision','avg_recall','avg_fscore','precision_correlation',\n",
    "              'asymptotic_precision','recall_correlation','asymptotic_recall','fscore_correlation','asymptotic_fscore']:\n",
    "        metrics['global'][m] = (np.average([metrics['fold_'+str(i+1)][m] for i in range(n_folds)]), \n",
    "                                np.std([metrics['fold_'+str(i+1)][m] for i in range(n_folds)]))\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset indicated by ``dataset`` (see above) and apply some basic cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/'+dataset+'.csv')\n",
    "data = data.fillna('')\n",
    "if dataset == 'webscope_r4':\n",
    "    data['labels'] = ['' if l == '\\\\N' else l for l in data['labels']]\n",
    "    data['labels'] = [l.replace('Action and Adventure', 'Action/Adventure') if 'Action and Adventure' in l else l for l in data['labels']]\n",
    "    data['labels'] = [l.replace('~Delete', '') if '~Delete' in l else l for l in data['labels']]\n",
    "    if select_only_labelled:\n",
    "        data = data.loc[data['labels'] != '']\n",
    "data['labels'] = [l.split(',') if l != '' else [] for l in data['labels']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_text_preprocessing:\n",
    "    data['text'] = [process_text(t) for t in data['text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print information about the label distribution and plot a bar chart with the number of samples per label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "n_labels_per_doc = []\n",
    "for labels in data['labels']:\n",
    "    n_labels_per_doc.append(len(labels))\n",
    "    if len(labels) > 0:\n",
    "        all_labels = all_labels + labels\n",
    "\n",
    "print('All labels:', np.unique(all_labels))\n",
    "print('Total n. of samples:', len(data))\n",
    "print('\\nNumber of unique labels:', len(np.unique(all_labels)))\n",
    "print('Cardinality (avg. n. of labels per doc):', round(np.average(n_labels_per_doc),2))\n",
    "print('Density (cardinality / |tot labels|):', round(np.average(n_labels_per_doc)/len(np.unique(all_labels)),2))\n",
    "print('Max. n. of labels per doc:', round(max(n_labels_per_doc),2))\n",
    "print('Min. n. of labels per doc:', round(min(n_labels_per_doc),2))\n",
    "\n",
    "n_labeled_docs = sum([1 for labels in data['labels'] if len(labels)>0])\n",
    "labeled_percentage = 100 * n_labeled_docs / len(data['labels'])\n",
    "print('Percentage of labeled docs:', round(labeled_percentage,2), '%')\n",
    "\n",
    "unique, counts = np.unique(all_labels, return_counts=True)\n",
    "label_frequencies = sorted(list(zip(unique, counts)), key=lambda x : x[-1], reverse=True)\n",
    "label_frequencies = label_frequencies[:20]\n",
    "unique = [l for l,c in label_frequencies]\n",
    "counts = [100*c/len(data['labels']) for l,c in label_frequencies]\n",
    "\n",
    "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "y = np.arange(len(unique))\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "ax.bar(y, counts, tick_label=['' for _ in unique])\n",
    "ax.set_ylim([0,21])\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_ylabel('Frequency (%)')\n",
    "#ax.set_title('Most frequent labels in the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tuples'] = data.apply(lambda row: (row['text'],row['labels']), axis=1)\n",
    "samples = data['tuples'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print information about the information ratio metrics (see the paper for references)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Zhu et al. 2018, Charte et al, 2015\n",
    "label_frequencies = calculate_label_frequencies([labels for text,labels in samples])\n",
    "label_occurrences = calculate_label_occurrences([labels for text,labels in samples])\n",
    "\n",
    "max_occurrence = max(label_occurrences.values())\n",
    "min_occurrence = min(label_occurrences.values())\n",
    "\n",
    "#IR\n",
    "maxIR = max_occurrence / min_occurrence\n",
    "print('MAX IR:', round(maxIR,2))\n",
    "\n",
    "#MeanIR and CVIR\n",
    "IR_per_label = [max_occurrence / label_occurrences[l] for l in label_occurrences]\n",
    "\n",
    "y = np.arange(len(IR_per_label))\n",
    "fig, ax = plt.subplots(figsize = (12, 6))\n",
    "ax.bar(y, sorted(IR_per_label, reverse=True))\n",
    "ax.set_xlabel('Labels')\n",
    "ax.set_ylabel('IR')\n",
    "#ax.set_title('Most frequent labels in the dataset')\n",
    "plt.show()\n",
    "\n",
    "meanIR = np.average(IR_per_label)\n",
    "stdevIR = np.std(IR_per_label)\n",
    "CVIR = stdevIR / meanIR\n",
    "print('Mean IR:', meanIR)\n",
    "print('CV IR:', CVIR)\n",
    "\n",
    "#LRID\n",
    "C = len(label_occurrences)\n",
    "N = len(samples)\n",
    "lrid = -2 * sum([label_occurrences[l] * math.log(N / (C*label_occurrences[l])) for l in label_occurrences])\n",
    "print('LRID:', round(lrid,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cross validation. See ``run_cross_validation`` for reference about the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = run_cross_validation(samples, n_folds=10, training_batch_size=8, n_training_iter=10, confidence_threshold=0.5,\n",
    "                               save_classifiers=False, load_classifiers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = []\n",
    "metric_list = ['meanIR','CVIR','lraps','lrl','cov_err','avg_exp','avg_balanced_accuracy','avg_precision','avg_recall','avg_fscore','precision_correlation',\n",
    "          'asymptotic_precision','recall_correlation','asymptotic_recall','fscore_correlation','asymptotic_fscore']\n",
    "for m in metric_list:\n",
    "    r = {}\n",
    "    for key in metrics:\n",
    "        r[key] = round(metrics[key][m],2) if key!='global' else str(round(metrics[key][m][0],2)) +' +- ' + str(round(metrics[key][m][1],2))\n",
    "    df_rows.append(r)\n",
    "    \n",
    "metrics_df = pd.DataFrame(df_rows, index=metric_list)     \n",
    "metrics_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (Eurecat)",
   "language": "python",
   "name": "eurecat-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
